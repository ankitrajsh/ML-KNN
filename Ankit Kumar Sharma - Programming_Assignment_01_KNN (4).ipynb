{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        "\n",
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment 01 - KNN : </b> Welcome to the programming assignment (PA) on k-nearest neighbors (KNN) classification. Throughout this PA, you will explore the k-NN algorithm, a versatile and intuitive method for tackling classification and regression challenges. Specifically, this assignment aims to enhance your understanding of the KNN classification algorithm. In this PA, we expect you to implement and experiment with the KNN classifier to understand how variations in 'k' and distance metrics influence classification performance.\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions are not accepted\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "<b> Part(a): Synthetic data generation </b>  \n",
        "\n",
        "1. Consider four bivariate Gaussians with means at (0,0), (0,1), (1,0), and (1,1), each having a variance of 0.3. Sample 90 points from each Gaussian, resulting in a total of 360 points. Allocate 30 points from each Gaussian for training and 60 points for testing. This results in a total of 120 points for training and 240 points for testing.\n",
        "2. Create a 2-class training set ($[X_{train}, Y_{train}]$) and a test set ($[X_{test}, Y_{test}]$) by labeling the data sampled from Gaussians with means at (0,0) and (1,1) as class 1, and the data sampled from Gaussians with means at (0,1) and (1,0) as class 2. Assign a label of +1 to class1 and -1 to class2.\n",
        "\n",
        "4. Visualize both train and test sets using the scatter plot on a 2-D plane. Indicate the data points from class 1 with a green color and those from class 2 with a blue color.\n",
        "\n",
        "<b> Part(b): KNN Classification - </b> The k-Nearest Neighbors (KNN) classifier algorithm is a straightforward yet powerful tool for classification tasks. The KNN classifier takes the test data point, computes distances to all points in the training set, identifies the 'k' nearest neighbors based on these distances, and assigns the test data to the class that the majority of its neighbors belong to.\n",
        "\n",
        "<b> Programming questions </b>\n",
        "\n",
        "\n",
        "1. Write a function called kNNClassify that accepts training data, a test point, and the hyperparameter 'k' as input and returns the label of the test point. Pick a reasonable 'k' for this experiment. Use \"kNNClassify\" function to generate the labels for the test data generated in part(a) of this PA. Compare the predicted labels with the original labels and calculate the portion of test data points that are correctly classified. In other words, calculate the accuracy of the classifier.\n",
        "3. Create a visual representation of the predictions by plotting all data points in a 2D plane. Assign green and blue colors to represent class 1 and class 2, respectively. For test data points that are misclassified, assign the color red.\n",
        "4. Generate and visualize the decision regions of 2D plane that are associated with each class, for a given classifier. Decision regions can be created by classifying all the data points in the 2D grid and assigning class-specific colors to them.\n",
        "\n",
        "<b> Part(c): Parameter selection: What is good value for k? - </b> One intuitive approach to determine the optimal 'k' is through cross-validation. During cross-validation, a ρ% portion of the training dataset is utilized as the validation dataset, and the model's performance is assessed on this validation set with various 'k' values. Through these cross-validation experiments, we select the 'k' that yields the best performance on the validation data.\n",
        "\n",
        "<b> Algorithm </b>\n",
        "\n",
        "1. Perform hold-out cross-validation by setting aside a fraction (ρ of the training set for validation. Note: You may use ρ = 0.3, and repeat the procedure 10 times. The hold-out procedure may be quite unstable.\n",
        "2. Use a large range of candidate values for k (e.g. k = 1, 3, 5..., 21). Notice odd numbers are considered to avoid ties.\n",
        "3. Repeat the process for 10 times using a random cross-validation set each time with a ρ = 0.3.\n",
        "4. Plot the training and validation errors for the different values of k.\n",
        "\n",
        "<b> Questions </b>\n",
        "\n",
        "5. How would you now answer the question \"what is the best value for k\"?\n",
        "6. How is the value of k affected by ρ (percentage of points held out) and number of repetitions? What does a large number of repetitions provide?\n",
        "7. Apply the model obtained by cross-validation (i.e., best k) to the test set and check\n",
        "if there is an improvement on the classification error over the result of Part 2.\n",
        "\n",
        "<b> Part(d): Influence of training data on KNN classifier - </b>\n",
        "\n",
        "1. Evaluate the performance as the size of the training set\n",
        "grows, e.g., n = {200, 400, 1200,...}. How would you choose a good range for k as n changes? What can you say about the stability of the solution? Check by repeating the validation multiple times.\n",
        "2. Try classifying more difficult datasets, for instance, by increasing the variance or adding noise by randomly flipping the labels on the training set.\n",
        "\n",
        "<b> Part(e): What is the influence of distance measure on decision regions ? - </b>\n",
        "\n",
        "1. Evaluate the performance of the KNN classifier with different distance measures such as $l_{1}$, $l_{2}$, etc,.\n",
        "2. Plot the decision regions of the KNN classifier with different distance measures.\n",
        "3. Report your observations.  \n",
        "\n",
        "<b> Part(f): MNIST Digit classification using KNN classifier: </b> : This part will not be graded. However, you are recommended to work on it to get exposure to the practical applications of the KNN classifier.\n",
        "\n",
        "1. Modify the function kNNClassify to handle multi-class problems and hence design a KNN classifier to classify the images in MNIST dataset as one of the 10 digits. The 28x28 images may be flattened to arrive at a 784 dimensional vector. NOTE: If you had already written a kNNClassify for multi class classification in part1 (b), you are free to use it.\n",
        "2. The MNIST dataset consists of approximately 70,000 images of handwritten digits. Create training, validation, and test datasets from this entire dataset with the respective proportions of 80%, 10%, and 10%.\n",
        "3. Empirically determine the most suitable error function, and the corresponding k to maximize the performance on the cross-validation experiments.\n",
        "4. Apply these values to evaluate the performance on the test dataset.\n",
        "5. Create a confusion matrix to understand the most confused classes (digits).\n",
        "6. Suggest alternate ways to improve the performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "gCAeXbdvgFVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part(a) #Synthetic data generation\n",
        "########################################\n",
        "#Define means and covariances\n",
        "mean1=\n",
        "mean2=\n",
        "mean3=\n",
        "mean4=\n",
        "cov=\n",
        "\n",
        "#Sample data points from the bivariate Gaussian distribution\n",
        "#You can use \"np.random.multivariate_normal\" function to sample the data points from the multivariate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Generate training data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Generate testing data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Visualize the data using plt.scatter() function\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjgkoGQOEjis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part(b) #KNN classification\n",
        "########################################\n",
        "\n",
        "#Write \"kNNClassify\" function\n",
        "def kNNClassify(X_train,Y_train,X_test,k):\n",
        "  '''\n",
        "  #Inputs : Training data (X_train,Y_train), Test points  (X_test), Hyperparameter k\n",
        "  #Outputs : Predicted class\n",
        "  '''\n",
        "  #Algorithm\n",
        "  #1.Initialize a list to store the predictions of our algorithm\n",
        "  #2.Iterate through the test points\n",
        "  #3.    Initialize a list to store the distances of test point with each of the training point.\n",
        "  #4.    Iterate through the training points\n",
        "  #5.        Compute the distance between the test and train point\n",
        "  #6.    Find out the k nearest neighbours\n",
        "  #7.    Get the most frequent label of k nearest neighbours\n",
        "  #8.Return the predictions\n",
        "  #Fill the code\n",
        "\n",
        "\n",
        "\n",
        "#Write \"KNNAccuracy\" function\n",
        "def KNNAccuracy(true,pred):\n",
        "  '''\n",
        "  #Inputs : Ground truth and predicted labels\n",
        "  #Outputs : Portion of data points that are correctly classified, i.e., accuracy\n",
        "  '''\n",
        "  #Algorithm\n",
        "  #1. Iterate through the total number of predictions\n",
        "  #2.     Verify wheter the ground truth matches with the predictions\n",
        "  #3. Compute and return the percentage of the correctly classified points\n",
        "  #Fill the code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a visual representation of predictions\n",
        "\n",
        "\n",
        "\n",
        "#Generate and visualize the decision regions and overlay the test points\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pH91uokhDV_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "#Part(c): Parameter selection: What is good value for k?\n",
        "####################################\n",
        "#Write holdoutCVkNN() Function\n",
        "def holdoutCVkNN(k_range,numrep,rho):\n",
        "  #Iterate through range of k values\n",
        "    #Iterate through number of repetations\n",
        "      #Hold out rho fraction of training data in each repetition.\n",
        "\n",
        "\n",
        "\n",
        "  #Return errors on training and validation data\n",
        "\n",
        "\n",
        "\n",
        "#Plot training and validation errors for different values of k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#what is the best value for k?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Effect of rho and number repetations on k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test set with the best hyper parameters ( i.e best k ).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EWmRWE8pDgnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(d): Influence of training data on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation as n increases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Experiments with more difficult data set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PG8vSBuoEEYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(e): Influence of distance measure on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation of KNN classifier with different distance measures\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Understand the decision regions of KNN classifier with different distance measures\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUub5YBDEfX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(f): Influence of distance measure on KNN classifier\n",
        "##################################\n",
        "\n",
        "#Load MNIST data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "images = mnist.data.to_numpy()\n",
        "targets = mnist.target.to_numpy()\n",
        "#Plot a few images\n",
        "plt.subplot(211)\n",
        "plt.imshow((images[0].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.subplot(212)\n",
        "plt.imshow(images[1].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "\n",
        "#Create train, validation and test splits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Write 'MultiClassKNNClassify' function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Empirically chose most suitable k and error function based on the evauation on cross-validation data\n",
        "#Evaluate the performance using different values of 'k' on the validation data and select the optimal 'k' for the test data.\n",
        "#Evaluation the performance using different distance measures (l1,l2, etc) on the validation data and select the optimal distance measure for test data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test data with the best hyper parameters ( k, error_func ) obtained from cross validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a confusion matrix for test data\n",
        "def compute_confusion_matrix(true, pred):\n",
        "    '''\n",
        "    Inputs: Ground truth labels and classifier predictions\n",
        "    Outputs: Confusion matrix\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "#Suggest an alternative ways to improve performance\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l1atXB22I7va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "1. Write down the best accuracy on synthetic test data generated from Gaussian distribution\n",
        "\n",
        "2. Write down the best accuracy on MNIST validation and test data.\n",
        "\n",
        "3. Report your observations on the confusion matrix of KNN classifier on MNIST test data\n",
        "\n",
        "4."
      ],
      "metadata": {
        "id": "OohdgUOoAenj"
      }
    }
  ]
}